{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2021 16:30:39 - INFO - faiss -   Loading faiss with AVX2 support.\n",
      "03/03/2021 16:30:39 - INFO - faiss -   Loading faiss.\n",
      "03/03/2021 16:30:39 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import pipeline, decomposition\n",
    "from sklearn import feature_extraction \n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import haystack.document_store.memory\n",
    "from haystack import document_store\n",
    "import haystack.retriever.dense\n",
    "from haystack import retriever\n",
    "\n",
    "import mlutil\n",
    "from mlutil.feature_extraction import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 150) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack library with scikit-learn extracted features\n",
    "\n",
    "Haystack is a library for question-answering and search that is mostly used with features extracted using word embeddings.\n",
    "\n",
    "Haystack supports basically two modes for using text features:\n",
    "- sparse, loading text for search using TF-IDF or BM25 using Elasticsearch\n",
    "- dense, with features extracted using huggingface transformers.\n",
    "\n",
    "This feature adds another way to add dense features: using scikit-learn.\n",
    "It might make sense to use this for fast prototyping (features like in Latent Semantic Analysis) or in low-resource environment where transformers would be a burden.\n",
    "\n",
    "Another motivation is using features extracted with word embeddings (like using PCREmbedding vectorizer from my [lambdaofgod/mlutil](https://github.com/lambdaofgod/mlutil) library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_train = dataset.data[:n_samples]\n",
    "data_test = dataset.data[n_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good old Latent Semantic Analysis\n",
    "\n",
    "We extract features by factorizing TF-IDF matrix with Truncated SVD.\n",
    "This can be done by creating scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_pipe = pipeline.make_pipeline(\n",
    "    feature_extraction.text.TfidfVectorizer(),\n",
    "    decomposition.TruncatedSVD(n_components=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.51 s, sys: 9.69 s, total: 13.2 s\n",
      "Wall time: 504 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('truncatedsvd',\n",
       "                 TruncatedSVD(algorithm='randomized', n_components=100,\n",
       "                              n_iter=5, random_state=None, tol=0.0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lsa_pipe.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_dim(embedder):\n",
    "    if isinstance(embedder, pipeline.Pipeline):\n",
    "        return embedder.steps[-1][1].components_.shape[0]\n",
    "    elif isinstance(embedder, mlutil.feature_extraction.embeddings.PCREmbeddingVectorizer):\n",
    "        return embedder.dimensionality_\n",
    "    else:\n",
    "        return embedder.components_.shape[0]\n",
    "\n",
    "\n",
    "def make_dicts_with_added_field(dicts, field_name, field_values):\n",
    "    for d, val in zip(dicts, field_values):\n",
    "        d[field_name] = val\n",
    "        yield d\n",
    "    \n",
    "\n",
    "def make_sklearn_retriever(df, pipeline, col='text'):\n",
    "    embedding_dim = get_embedding_dim(pipeline)\n",
    "    memory_docstring_store = document_store.memory.InMemoryDocumentStore(index=col, embedding_dim=embedding_dim)\n",
    "    df['text'] = df[col]\n",
    "    embeddings = pipeline.fit_transform(df['text'])\n",
    "    documents = df.to_dict('records')\n",
    "    dicts_with_added_fields = list(make_dicts_with_added_field(documents, 'embedding', embeddings))\n",
    "    memory_docstring_store.write_documents(dicts_with_added_fields)\n",
    "    return retriever.dense.SklearnTransformerRetriever(embedding_transformer=pipeline, document_store=memory_docstring_store)\n",
    "\n",
    "\n",
    "def prettify_response(response):\n",
    "    return pd.Series([ \n",
    "        doc.text for doc in response\n",
    "    ]).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'newsgroups_text': data_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df['newsgroups_text'].apply(len) > 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_pipe.fit(df['newsgroups_text']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_retriever = make_sklearn_retriever(df, lsa_pipe, col='newsgroups_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Well, this is alt.atheism.  I hope you arent here to try to convert anyone.\\n\\n\\nMany would disagree.\\n\\n[...]\\n\\nWell, you shouldn't give any par...\n",
       "1    Yeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs ...\n",
       "2    As I was created in the image of Gaea, therefore I must\\nbe the pinnacle of creation, She which Creates, She which\\nBirths, She which Continues.\\n...\n",
       "3    Are you your own master?  Do you have any habits that you cannot break?\\nFor one, you seem unable to master your lack of desire to understand\\neve...\n",
       "4                                                                                                                                                         \n",
       "5    [..]\\nReferring to the manual of my motherboard with AMI-BIOS, 10 beeps are a \\n'CMOS Shutdown Register Read/Write Error', if the system stops aft...\n",
       "6    I posted about this a while ago but without code excerpts noone was\\nable to help me.\\n\\nThe problem is that main_win.win is doing fine, but when ...\n",
       "7    A while back someone had several equations which could be used for changing 3 f\\niltered grey scale images into one true color image.  This is pos...\n",
       "8    I'm sorry, I thought we were discussing heresy.  I assumed that heresy\\nmeant a departure from orthodoxy, in which case generally accepted belief ...\n",
       "9    It was a gift from God.  I think basically the reasoning was that the\\ntradition in the Church held that Mary was also without sin as was Jesus.\\n...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prettify_response(lsa_retriever.retrieve('atheism'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: PCR embeddings vectorizer\n",
    "\n",
    "For detailed description of method check out [A Critique of the Smooth Inverse Frequency Sentence Embeddings](https://arxiv.org/pdf/1909.13494.pdf)\n",
    "\n",
    "PCR Vectorizer [implementation link](https://github.com/lambdaofgod/mlutil/blob/master/mlutil/feature_extraction/embeddings.py#L179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/03/2021 16:30:41 - INFO - gensim.models.utils_any2vec -   loading projection weights from /home/kuba/gensim-data/glove-twitter-50/glove-twitter-50.gz\n",
      "03/03/2021 16:31:17 - INFO - gensim.models.utils_any2vec -   loaded (1193514, 50) matrix from /home/kuba/gensim-data/glove-twitter-50/glove-twitter-50.gz\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = embeddings.load_gensim_embedding_model('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_vectorizer = embeddings.PCREmbeddingVectorizer(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_vectorizer.fit(df['newsgroups_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newsgroups_text_pcr'] = df['newsgroups_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pcr_vectorizer.transform(df['newsgroups_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_retriever = make_sklearn_retriever(df, pcr_vectorizer, col='newsgroups_text_pcr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I'm sorry, I thought we were discussing heresy.  I assumed that heresy\\nmeant a departure from orthodoxy, in which case generally accepted belief ...\n",
       "1            True.\\n\\nAlso read 2 Peter 3:16\\n\\nPeter warns that the scriptures are often hard to understand by those who\\nare not learned on the subject.\n",
       "2    Exactly.\\n\\nBut I'll add another observation: if the chip does become a standard,\\nthe algorithm won't _remain_ secret.\\n\\nLeaving the government ...\n",
       "3    \"Put not your trust in princes\" is the Biblical proverb.  The modern\\nanalog is governments.  At the time of the founding of the US, the\\nidea tha...\n",
       "4    You're admitting a lot more than that.  You are admitting that\\nyour morals are situational.   You are admitting that the actions\\nof other people...\n",
       "5    Yes it is, as has been evidenced by the previous two stages\\nof withdrawal from the area and by the reductions in troops.\\nCurrently the troops ar...\n",
       "6    Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort ab...\n",
       "7    Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels repu...\n",
       "8    OK, I've asked this before, and with a new thread on these lines, I\\nask this again:\\n\\n1: If a large hole current is run thru a resistor, will th...\n",
       "9    Hi Damon,  No matter what system or explanation of creation you wish\\nto accept, you always have to start with one of two premises, creation\\nfrom...\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prettify_response(pcr_retriever.retrieve('atheism'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
